# rvqa

The Relation-VQA data will be released soon.

If you have any question, please do not hesitate to contact me.


## Relation-VQA Dataset


- [vqa-rel_map_result_0.30.txt](https://drive.google.com/open?id=1JXf5NqkhgffMsBz7skjtJOYRZyad9bj-)
- [vqa-rel_map_result_0.30.json](https://drive.google.com/open?id=1WhI7MO8Ml9nAvKb0TFsUapFis88_SBcx)


### Reference
- **Paper on SIGKDD**: http://www.kdd.org/kdd2018/accepted-papers/view/r-vqa-learning-visual-relation-facts-with-semantic-attention-for-visual-que
- **Paper on arXiv**: https://arxiv.org/abs/1805.09701



If you use this project as part of any published research, please acknowledge the following paper.
```
@inproceedings{lu2018rvqa,
	title={R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering.},
	author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
	booktitle={SIGKDD 2018},
	year={2018}
}
```
